<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <!-- <meta name="title" content="PAPER_TITLE - AUTHOR_NAMES"> -->
  <meta name="title" content="Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty‑Aware Sim‑to‑Real Manipulation - Maggie Wang, Stephen Tian, Swann, Ola Shorinwa, Jiajun Wu, Mac Schwager" />
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="Phys2Real fuses VLM priors with online interaction to estimate physical parameters and adapt RL policies, improving sim‑to‑real manipulation on T‑block and hammer pushing tasks.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="sim-to-real transfer, robotic manipulation, reinforcement learning, rapid motor adaptation, RMA, vision-language models, VLM, 3D Gaussian splatting, GSplat, digital twin, uncertainty fusion, system identification">
  <!-- TODO: List all authors -->
  <meta name="author" content="Maggie Wang, Stephen Tian, Aiden Swann, Ola Shorinwa, Jiajun Wu, Mac Schwager">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="Multi-Robot Systems Lab">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty‑Aware Sim‑to‑Real Manipulation">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="Phys2Real fuses VLM priors with online interaction to adapt RL policies for robust sim‑to‑real manipulation.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://maggiewang.org/phys2real">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://maggiewang.org/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty‑Aware Sim‑to‑Real Manipulation - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="Maggie Wang">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="PAPER_TITLE">
  <meta name="citation_author" content="FIRST_AUTHOR_LAST, FIRST_AUTHOR_FIRST">
  <meta name="citation_author" content="SECOND_AUTHOR_LAST, SECOND_AUTHOR_FIRST">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <!-- <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> -->
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=PT+Sans:wght@400;700&display=swap" rel="stylesheet">


  <!-- TODO: Replace with your paper title and authors -->
  <title>Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty-Aware Sim-to-Real Manipulation - AUTHOR_NAMES | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/phys2real-3.ico">
  <link rel="apple-touch-icon" href="static/images/phys2real-3.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <!-- <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet"> -->
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>

<main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">

            <!-- Title row (icon + title) -->
            <div style="
              display:flex;
              align-items:flex-start;
              justify-content:center;
              gap: 0.8rem;
              flex-wrap: nowrap;
              max-width: 1200px;
              margin: 0 auto 0.25rem;   /* << tighten space under title row */
            ">
              <img
                src="static/images/phys2real.gif"
                alt="Phys2Real animated logo"
                style="
                  width: 102px;
                  height: auto;
                  opacity: 0.95;
                  filter: drop-shadow(0 1px 4px rgba(0,0,0,0.05));
                  background-color: rgba(255,255,255,0.8);
                  border-radius: 8px;
                  flex: 0 0 auto;
                  margin-top: 3px;
                "
              />
              <h1 class="title publication-title"
                  style="
                    font-size: 2.8rem;
                    font-weight: 700;
                    line-height: 1.18;   /* slightly tighter lines */
                    margin: 0;           /* kill default title margins */
                    text-align: left;
                    max-width: 1050px;
                  ">
                <span style="font-weight: 900; color: #0a0a0a; letter-spacing: -0.5px;">Phys2Real</span>:
                Fusing VLM Priors with Interactive Online Adaptation for
                Uncertainty-Aware Sim-to-Real Manipulation
              </h1>
            </div>

            <!-- Authors (still inside the same column) -->
            <div class="is-size-5 publication-authors has-text-centered"
                 style="margin-top: -0.15rem;">  <!-- << pull up into the title -->
              <span class="author-block"><a href="https://maggiewang.org/about" target="_blank" rel="noopener">Maggie Wang</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://s-tian.github.io/" target="_blank" rel="noopener">Stephen Tian</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://aidenswann.com/" target="_blank" rel="noopener">Aiden Swann</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://www.researchgate.net/scientific-contributions/Ola-Shorinwa-2173398990" target="_blank" rel="noopener">Ola Shorinwa</a><sup>2</sup>,</span>
              <span class="author-block"><a href="https://jiajunwu.com" target="_blank" rel="noopener">Jiajun Wu</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://web.stanford.edu/~schwager/" target="_blank" rel="noopener">Mac Schwager</a><sup>1</sup></span>
            </div>

            <!-- Affiliations -->
            <div class="is-size-6 affiliations has-text-centered" style="margin-top: 0.25rem;">
              <span class="affil-block"><sup>1</sup> Stanford University</span>
              <span class="affil-block"><sup>2</sup> Princeton University</span>
            </div>

            <!-- Buttons -->
            <div class="column has-text-centered" style="margin-top: 0.75rem;">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/YOUR REPO HERE" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Code [coming soon!]</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="ai ai-arxiv"></i></span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>

          </div> <!-- /column -->
        </div>   <!-- /columns -->
      </div>     <!-- /container -->
    </div>
  </section>
</main>


<!-- Overview Figure -->
<!-- <section class="hero is-light" id="overview-banner">
  <div class="hero-body has-text-centered">
    <div class="container is-max-desktop">
      <figure class="image">
        <img src="static/images/pipeline.png" alt="Phys2Real overview figure" id="overview-image">
      </figure>
      <p class="is-size-6 has-text-grey mt-4" style="max-width: 800px; margin: 0 auto;">
        <strong>Phys2Real</strong> is a real-to-sim-to-real pipeline for robotic manipulation that combines
        VLM-based physical parameter estimation with interaction-based adaptation through
        uncertainty-aware fusion. It comprises three stages:<br>
        (I) <em>real-to-sim</em>: object reconstruction from segmented Gaussian Splats into
        simulation-ready meshes;<br>
        (II) <em>policy learning</em>: reinforcement learning of policies conditioned on physical
        parameters such as the center of mass (CoM) of an object; and<br>
        (III) <em>sim-to-real transfer</em>: uncertainty-aware fusion of VLM priors and
        interaction-based estimates for online adaptation.
      </p>
    </div>
  </div>
</section> -->
<!-- End Overview Figure -->

<!-- Overview Figure -->
<section class="hero is-light" id="overview-banner">
  <div class="hero-body has-text-centered">
    <div class="container" style="max-width: 1050px;">
      <figure class="image" style="margin-bottom: 1.25rem;">
        <img 
          src="static/images/pipeline.png" 
          alt="Phys2Real overview figure" 
          id="overview-image"
          style="width: 100%; height: auto; border-radius: 6px; box-shadow: 0 2px 12px rgba(0,0,0,0.06);">
      </figure>

      <div class="content" style="max-width: 850px; margin: 0 auto;">
        <p class="is-size-6 has-text-grey-dark has-text-justified" style="margin-bottom: 0.75rem;">
          <strong>Phys2Real</strong> is a real-to-sim-to-real pipeline for robotic manipulation that 
          combines VLM-based physical parameter estimation with interaction-based adaptation 
          through uncertainty-aware fusion. It comprises three stages:
        </p>

        <ol class="is-size-6 has-text-grey has-text-left" style="margin: 0 auto; max-width: 800px; line-height: 1.5;">
          <li><strong>Real-to-Sim</strong>: object reconstruction from segmented Gaussian Splats into simulation-ready meshes.</li>
          <li><strong>Policy Learning</strong>: reinforcement learning of policies conditioned on physical parameters such as the center of mass (CoM) of an object.</li>
          <li><strong>Sim-to-Real Transfer</strong>: <em>uncertainty-aware</em> fusion of VLM priors and interaction-based estimates for online adaptation.</li>
        </ol>
      </div>
    </div>
  </div>
</section>
<!-- End Overview Figure -->


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            Learning robotic manipulation policies directly in the real world can be expensive and time-consuming. While reinforcement learning (RL) policies trained in simulation present a scalable alternative, effective sim-to-real transfer remains challenging, particularly for tasks that require precise dynamics. To address this, we propose <b>Phys2Real</b>, a real-to-sim-to-real RL pipeline that combines vision-language model (VLM)-inferred physical parameter estimates with interactive adaptation through uncertainty-aware fusion. Our approach consists of three core components: (1) high-fidelity geometric reconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions over physical parameters, and (3) online physical parameter estimation from interaction data. Phys2Real conditions policies on interpretable physical parameters, refining VLM predictions with online estimates via ensemble-based uncertainty quantification. On planar pushing tasks of a T-block with varying CoM and a hammer with an off-center mass distribution, Phys2Real achieves substantial improvements over a domain randomization baseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23% in the challenging top-weighted T-block, and 15% faster task completion for hammer pushing. Ablation studies indicate that the combination of VLM and interaction information is essential for success.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section">
  <div class="container is-max-desktop has-text-centered" style="max-width: 900px;">
    <p class="is-size-4 has-text-weight-medium line-tight">
      We fuse <b>VLM priors</b> with <b><i>interactive</i> online adaptation</b><br>
      to estimate physical parameters and improve sim-to-real manipulation.
    </p>

    <p class="is-size-5 has-text-grey-dark" style="margin-top: 0.5rem;">
      Uncertainty-aware fusion of VLM priors and interaction<br>
      yields near-privileged sim-to-real performance on planar pushing.
    </p>

    <div class="tags is-centered mt-4 tldr-tags">
      <span class="tag is-info is-light">
        <b>T-block (with weight at <i>top</i> of block)</b>: 57% vs 24% DR
      </span>
      <span class="tag is-info is-light">
        <b>T-block (with weight at <i>bottom</i> of block)</b>: 100% vs 79% DR
      </span>
      <span class="tag is-info is-light">
        <b>Hammer</b>: 15–17% faster than DR
      </span>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- TODO: Replace with your teaser video -->
      <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata">
        <!-- TODO: Add your video file path here -->
        <source src="static/videos/phys2real_vid_final.mp4" type="video/mp4">
      </video>
      <!-- TODO: Replace with your video description -->
      <h2 class="subtitle has-text-centered">
        <!-- Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.  -->
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Physically-Informed Digital Twin</h2>
    <div class="content has-text-justified">
      <p>
        We reconstruct simulation-ready assets from real scenes: segmented 3D Gaussian splats are converted into watertight meshes. This “real-to-sim” step lets policies train on objects that capture geometry relevant to dynamics.
      </p>
    </div>
    <figure class="image publication-banner">
      <!-- Replace with your export, e.g., static/images/reconstruction.png -->
      <img src="static/images/realtosim.png" alt="Pipeline: GSplat segmentation → mesh extraction → watertight simulation mesh">
    </figure>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Policy Learning & Online Adaptation</h2>

    <div class="columns is-multiline is-vcentered">
      <div class="column is-6">
        <div class="content has-text-justified">
          <p>
            We train policies conditioned on interpretable physical parameters (e.g., the object’s CoM). Training proceeds in three phases: (1) privileged training with accurate parameters, (1.5) noise-aware fine-tuning for robustness, and (2) online adaptation via an uncertainty-aware ensemble that updates the parameter belief during execution.
          </p>
        </div>
      </div>
      <div class="column is-6">
        <figure class="image publication-banner">
          <!-- Replace with your export, e.g., static/images/training_phases.png -->
          <img src="static/images/phys2real_phases.png" alt="Phase 1–1.5–2 policy training and adaptation">
        </figure>
      </div>
    </div>

    <div class="columns is-multiline is-vcentered mt-4">
      <div class="column is-6">
        <figure class="image publication-banner">
          <!-- Replace with your export, e.g., static/images/vlm_prior.png -->
          <img src="static/images/vlm_prompts.png" alt="VLM prior estimation for physical parameters">
        </figure>
      </div>
      <div class="column is-6">
        <div class="content has-text-justified">
          <p>
            A vision-language model provides a <em>prior</em> distribution over the parameters (mean + uncertainty). During interaction, an RMA-style estimator refines this belief. We fuse them using their uncertainties so the policy sees a single coherent estimate that improves over time.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- 1) What we ask -->
<section class="section" id="research-questions">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">How Do VLM Priors, Interaction, and Their Fusion <br>Each Contribute to Sim-to-Real Manipulation?</h2>
    <!-- <p class="has-text-grey has-text-centered" style="max-width:820px;margin:0 auto 1rem;">
      How do priors, interaction, and their fusion each contribute to sim-to-real manipulation?
    </p> -->

    <div class="columns is-multiline">
      <div class="column is-6">
        <div class="box">
          <span class="tag is-link is-light">Q1</span>
          <h3 class="title is-5" style="margin:.5rem 0 0.25rem;">Do accurate physical parameters improve performance?</h3>
          <p>Compare a <b>privileged (ground-truth)</b> physics-conditioned policy to one unaware of physical properties.</p>
        </div>
      </div>

      <div class="column is-6">
        <div class="box">
          <span class="tag is-link is-light">Q2</span>
          <h3 class="title is-5" style="margin:.5rem 0 0.25rem;">Are VLM priors alone sufficient?</h3>
          <p>Condition the policy only on <b>VLM-estimated parameters</b>, without interaction.</p>
        </div>
      </div>

      <div class="column is-6">
        <div class="box">
          <span class="tag is-link is-light">Q3</span>
          <h3 class="title is-5" style="margin:.5rem 0 0.25rem;">Is interaction alone sufficient?</h3>
          <p>Remove the VLM prior and use only the <b>RMA-based online estimator</b>.</p>
        </div>
      </div>

      <div class="column is-6">
        <div class="box">
          <span class="tag is-link is-light">Q4</span>
          <h3 class="title is-5" style="margin:.5rem 0 0.25rem;">Does Phys2Real generalize to real-world objects?</h3>
          <p>Apply to <b>image-reconstructed, mesh-free objects</b> (e.g., hammer).</p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- 2) Answer each question with figures -->
<section class="section" id="main-results">
  <div class="container is-max-desktop">

    <!-- Q1–Q3: T-block -->
    <h2 class="title is-3 has-text-centered">Hardware Experiments</h2>

    <div class="box">
      <p class="is-size-6">
        <span class="tag is-link is-light">Q1–Q3</span>
        <b>T-block pushing:</b> Phys2Real <b>consistently outperforms DR and diffusion</b>. With only VLM priors + interaction, we
        approach privileged performance while staying interpretable.
      </p>

      <div class="columns">
        <div class="column">
          <figure class="image publication-banner">
            <img src="static/images/position_error_cdf_weighttop_10cm.jpg" alt="CDF for T-block with top weight">
          </figure>
        </div>
        <div class="column">
          <figure class="image publication-banner">
            <img src="static/images/position_error_cdf_weightbottom_5cm.jpg" alt="CDF for T-block with bottom weight">
          </figure>
        </div>
      </div>

      <p class="has-text-grey is-size-7 has-text-centered" style="margin-top:.25rem;">
        CDF = fraction of trials below each position error. Higher & further left is better.
        Example: 0.8 at 2&nbsp;cm ⇒ 80% of trials ≤ 2&nbsp;cm.
      </p>

      <div class="notification is-light is-info" style="margin-top:.5rem;">
        <b>Takeaway.</b> Neither VLM-only nor interaction-only suffices; <u>their uncertainty-aware fusion</u> is what closes the gap.
      </div>

      <p class="has-text-grey">
        Top weight: 24% (DR) → 57% (Phys2Real). Bottom weight: 79% → 100%.
      </p>
    </div>

    <!-- Q4: Hammer -->
    <div class="box">
      <p class="is-size-6">
        <span class="tag is-link is-light">Q4</span>
        <b>Hammer (reconstructed object):</b> both methods succeed, but Phys2Real is ~<b>16% faster</b> to completion.
      </p>

      <div class="columns is-centered">
        <div class="column is-6">
          <figure class="image publication-banner">
            <img src="static/images/hammer_completion_time_comparison.png" alt="Hammer completion time comparison">
          </figure>
        </div>
      </div>

      <!-- Side-by-side hammer videos: Phys2Real vs DR (square) -->
      <div class="columns is-centered is-variable is-5"
           style="margin-top:.75rem; max-width:720px; margin-left:auto; margin-right:auto;">
        <!-- Phys2Real -->
        <div class="column is-6">
          <figure class="image publication-banner" style="margin:0;">
            <video
              class="publication-video-el"
              controls
              autoplay
              muted
              loop
              playsinline
              preload="metadata"
              style="width:100%; aspect-ratio:1/1; border-radius:12px; object-fit:contain;">
              <source src="static/images/hammer_phys2real.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </figure>
          <p class="has-text-centered is-size-7" style="margin-top:.4rem;">
            <b>Phys2Real</b> — faster completion
          </p>
        </div>

        <!-- DR -->
        <div class="column is-6">
          <figure class="image publication-banner" style="margin:0;">
            <video
              class="publication-video-el"
              controls
              autoplay
              muted
              loop
              playsinline
              preload="metadata"
              style="width:100%; aspect-ratio:1/1; border-radius:12px; object-fit:contain;">
              <source src="static/images/hammer_dr.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </figure>
          <p class="has-text-centered is-size-7" style="margin-top:.4rem;">
            <b>Domain Randomization (DR)</b>
          </p>
        </div>
      </div>



    </div>

    <!-- Optional: one-line synthesis -->
    <p class="has-text-grey has-text-centered">
      <em>Vision provides a prior; interaction reduces uncertainty for robust sim-to-real control.</em>
    </p>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">
      How VLM Priors and Interactive Online Adaptation <br>Fusion Evolves Over Time
    </h2>
    <div class="content has-text-justified">
      <p>
        Early in an episode, the fused estimate relies on the VLM prior. As interaction accumulates, the ensemble’s epistemic uncertainty shrinks and the fused estimate converges toward the ground truth. This makes the policy’s behavior predictable and debuggable.
      </p>
    </div>

    <!-- Top-weighted block -->
    <h3 class="title is-4 has-text-centered mt-6">T-Block (Weight on Top)</h3>
    <figure class="image publication-banner">
      <video
        class="publication-video-el"
        controls
        autoplay
        muted
        loop
        playsinline
        preload="metadata"
        poster="static/images/fusion_over_time_top_poster.jpg">
        <source src="static/images/episode_20250912-131707_paper_animation.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </figure>

    <!-- Bottom-weighted block -->
    <h3 class="title is-4 has-text-centered mt-6">T-Block (Weight on Bottom)</h3>
    <figure class="image publication-banner">
      <video
        class="publication-video-el"
        controls
        autoplay
        muted
        loop
        playsinline
        preload="metadata"
        poster="static/images/fusion_over_time_bottom_poster.jpg">
        <source src="static/images/episode_20250912-131707_paper_animation.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </figure>
  </div>
</section>

<section class="section" id="takeaway">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Toward Physically Grounded Foundation Models</h2>
    <div class="content has-text-justified" style="max-width:850px;margin:0 auto;">
      <p>
        <b>Phys2Real</b> shows that combining <b>foundation model priors</b> with <b>interactive online adaptation</b>
        yields physically grounded, uncertainty-aware policies that transfer more reliably to the real world.
        By treating large vision models as sources of physical intuition and refining those estimates through real interaction,
        we move toward robotic systems that can <i>understand, predict, and adapt</i> in diverse environments.
      </p>
    </div>
  </div>
</section>



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
   <!--    <pre id="bibtex-code"><code>@article{YourPaperKey2024,
  title={Your Paper Title Here},
  author={First Author and Second Author and Third Author},
  journal={Conference/Journal Name},
  year={2024},
  url={https://your-domain.com/your-project-page}
}</code></pre> -->
<pre id="bibtex-code"><code>@article{wang2025phys2real,
  title     = {Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty-Aware Sim-to-Real Manipulation},
  author    = {Wang, Maggie and Tian, Stephen and Swann, Aiden and Shorinwa, Ola and Wu, Jiajun and Schwager, Mac},
  journal   = {arXiv preprint arXiv:2501.XXXX},
  year      = {2025},
  url       = {https://arxiv.org/abs/2501.XXXX}
}}</code></pre>
    </div>
</section>
<!--End BibTex citation -->

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
