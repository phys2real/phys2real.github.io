<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty‑Aware Sim‑to‑Real Manipulation - Maggie Wang, Stephen Tian, Aiden Swann, Ola Shorinwa, Jiajun Wu, Mac Schwager" />
  <meta name="description" content="Phys2Real fuses VLM priors with online interaction to estimate physical parameters and adapt RL policies, improving sim‑to‑real manipulation on T‑block and hammer pushing tasks.">
  <meta name="keywords" content="sim-to-real transfer, robotic manipulation, reinforcement learning, rapid motor adaptation, RMA, vision-language models, VLM, 3D Gaussian splatting, GSplat, digital twin, uncertainty fusion, system identification">
  <meta name="author" content="Maggie Wang, Stephen Tian, Aiden Swann, Ola Shorinwa, Jiajun Wu, Mac Schwager">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Multi-Robot Systems Lab">
  <meta property="og:title" content="Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty‑Aware Sim‑to‑Real Manipulation">
  <meta property="og:description" content="Phys2Real fuses VLM priors with online interaction to adapt RL policies for robust sim‑to‑real manipulation.">
  <meta property="og:url" content="https://maggiewang.org/phys2real">
  <meta property="og:image" content="https://maggiewang.org/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty‑Aware Sim‑to‑Real Manipulation - Research Preview">
  <meta property="article:published_time" content="2025-01-01T00:00:00.000Z">
  <meta property="article:author" content="Maggie Wang">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="sim-to-real">
  <meta property="article:tag" content="robotic manipulation">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@StanfordMSL">
  <meta name="twitter:creator" content="@maggi3wang_">
  <meta name="twitter:title" content="Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty‑Aware Sim‑to‑Real Manipulation">
  <meta name="twitter:description" content="Phys2Real fuses VLM priors with online interaction to estimate physical parameters and adapt RL policies, achieving 100% vs 79% success on weighted T-block manipulation.">
  <meta name="twitter:image" content="https://maggiewang.org/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="Phys2Real: Fusing VLM Priors with Interactive Online Adaptation - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty‑Aware Sim‑to‑Real Manipulation">
  <meta name="citation_author" content="Wang, Maggie">
  <meta name="citation_author" content="Tian, Stephen">
  <meta name="citation_author" content="Swann, Aiden">
  <meta name="citation_author" content="Shorinwa, Ola">
  <meta name="citation_author" content="Wu, Jiajun">
  <meta name="citation_author" content="Schwager, Mac">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="arXiv preprint">
  <meta name="citation_pdf_url" content="https://phys2real.github.io/static/pdfs/phys2real.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=PT+Sans:wght@400;700&display=swap" rel="stylesheet">

  <title>Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty-Aware Sim-to-Real Manipulation | Stanford Multi-Robot Systems Lab</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/phys2real-3.ico">
  <link rel="apple-touch-icon" href="static/images/phys2real-3.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty‑Aware Sim‑to‑Real Manipulation",
    "description": "A real-to-sim-to-real RL pipeline that combines VLM-inferred physical parameter estimates with interactive adaptation through uncertainty-aware fusion for improved robotic manipulation.",
    "author": [
      {
        "@type": "Person",
        "name": "Maggie Wang",
        "affiliation": {
          "@type": "Organization",
          "name": "Stanford University"
        }
      },
      {
        "@type": "Person",
        "name": "Stephen Tian",
        "affiliation": {
          "@type": "Organization",
          "name": "Stanford University"
        }
      },
      {
        "@type": "Person",
        "name": "Aiden Swann",
        "affiliation": {
          "@type": "Organization",
          "name": "Stanford University"
        }
      },
      {
        "@type": "Person",
        "name": "Ola Shorinwa",
        "affiliation": {
          "@type": "Organization",
          "name": "Princeton University"
        }
      },
      {
        "@type": "Person",
        "name": "Jiajun Wu",
        "affiliation": {
          "@type": "Organization",
          "name": "Stanford University"
        }
      },
      {
        "@type": "Person",
        "name": "Mac Schwager",
        "affiliation": {
          "@type": "Organization",
          "name": "Stanford University"
        }
      }
    ],
    "datePublished": "2025-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "arXiv"
    },
    "url": "https://phys2real.github.io/",
    "image": "https://phys2real.github.io//static/images/social_preview.png",
    "keywords": ["sim-to-real transfer", "robotic manipulation", "vision-language models", "uncertainty quantification", "reinforcement learning", "3D Gaussian splatting"],
    "abstract": "Learning robotic manipulation policies directly in the real world can be expensive and time-consuming. While reinforcement learning (RL) policies trained in simulation present a scalable alternative, effective sim-to-real transfer remains challenging, particularly for tasks that require precise dynamics. To address this, we propose Phys2Real, a real-to-sim-to-real RL pipeline that combines vision-language model (VLM)-inferred physical parameter estimates with interactive adaptation through uncertainty-aware fusion.",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://phys2real.github.io/"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Robotic Manipulation"
      },
      {
        "@type": "Thing", 
        "name": "Sim-to-Real Transfer"
      },
      {
        "@type": "Thing",
        "name": "Vision-Language Models"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Stanford Multi-Robot Systems Lab",
    "url": "https://msl.stanford.edu",
    "logo": "https://phys2real.github.io/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/StanfordMSL",
      "https://github.com/StanfordMSL"
    ]
  }
  </script>
</head>
<body>

<main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">

            <!-- Title row (icon + title) -->
            <div style="
              display:flex;
              align-items:flex-start;
              justify-content:center;
              gap: 0.8rem;
              flex-wrap: nowrap;
              max-width: 1200px;
              margin: 0 auto 0.25rem;   /* << tighten space under title row */
            ">
              <img
                src="static/images/phys2real.gif"
                alt="Phys2Real animated logo"
                style="
                  width: 102px;
                  height: auto;
                  opacity: 0.95;
                  filter: drop-shadow(0 1px 4px rgba(0,0,0,0.05));
                  background-color: rgba(255,255,255,0.8);
                  border-radius: 8px;
                  flex: 0 0 auto;
                  margin-top: 3px;
                "
              />
              <h1 class="title publication-title"
                  style="
                    font-size: 2.8rem;
                    font-weight: 700;
                    line-height: 1.18;   /* slightly tighter lines */
                    margin: 0;           /* kill default title margins */
                    text-align: left;
                    max-width: 1050px;
                  ">
                <span style="font-weight: 900; color: #0a0a0a; letter-spacing: -0.5px;">Phys2Real</span>:
                Fusing VLM Priors with Interactive Online Adaptation for
                Uncertainty-Aware Sim-to-Real Manipulation
              </h1>
            </div>

            <!-- Authors (still inside the same column) -->
            <div class="is-size-5 publication-authors has-text-centered"
                 style="margin-top: -0.15rem;">  <!-- << pull up into the title -->
              <span class="author-block"><a href="https://maggiewang.org/about" target="_blank" rel="noopener">Maggie Wang</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://s-tian.github.io/" target="_blank" rel="noopener">Stephen Tian</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://aidenswann.com/" target="_blank" rel="noopener">Aiden Swann</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://www.researchgate.net/scientific-contributions/Ola-Shorinwa-2173398990" target="_blank" rel="noopener">Ola Shorinwa</a><sup>2</sup>,</span>
              <span class="author-block"><a href="https://jiajunwu.com" target="_blank" rel="noopener">Jiajun Wu</a><sup>1</sup>,</span>
              <span class="author-block"><a href="https://web.stanford.edu/~schwager/" target="_blank" rel="noopener">Mac Schwager</a><sup>1</sup></span>
            </div>

            <!-- Affiliations -->
            <div class="is-size-6 affiliations has-text-centered" style="margin-top: 0.25rem;">
              <span class="affil-block"><sup>1</sup> Stanford University</span>
              <span class="affil-block"><sup>2</sup> Princeton University</span>
            </div>

            <!-- Buttons -->
            <div class="column has-text-centered" style="margin-top: 0.75rem;">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2501.XXXXX.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://github.com/StanfordMSL/phys2real" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Code [coming soon!]</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2501.XXXXX" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="ai ai-arxiv"></i></span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>

          </div> <!-- /column -->
        </div>   <!-- /columns -->
      </div>     <!-- /container -->
    </div>
  </section>
</main>

<!-- Overview Figure -->
<section class="hero is-light" id="overview-banner">
  <div class="hero-body has-text-centered">
    <div class="container" style="max-width: 1050px;">
      <figure class="image" style="margin-bottom: 1.25rem;">
        <img 
          src="static/images/pipeline.png" 
          alt="Phys2Real overview figure" 
          id="overview-image"
          style="width: 100%; height: auto; border-radius: 6px; box-shadow: 0 2px 12px rgba(0,0,0,0.06);">
      </figure>

      <div class="content" style="max-width: 850px; margin: 0 auto;">
        <p class="is-size-6 has-text-grey-dark has-text-justified" style="margin-bottom: 0.75rem;">
          <strong>Phys2Real</strong> is a real-to-sim-to-real pipeline for robotic manipulation that 
          combines VLM-based physical parameter estimation with interaction-based adaptation 
          through uncertainty-aware fusion. It comprises three stages:
        </p>

        <ol class="is-size-6 has-text-grey has-text-left" style="margin: 0 auto; max-width: 800px; line-height: 1.5;">
          <li><strong>Real-to-Sim</strong>: object reconstruction from segmented Gaussian Splats into simulation-ready meshes.</li>
          <li><strong>Policy Learning</strong>: reinforcement learning of policies conditioned on physical parameters such as the center of mass (CoM) of an object.</li>
          <li><strong>Sim-to-Real Transfer</strong>: <em>uncertainty-aware</em> fusion of VLM priors and interaction-based estimates for online adaptation.</li>
        </ol>
      </div>
    </div>
  </div>
</section>
<!-- End Overview Figure -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Learning robotic manipulation policies directly in the real world can be expensive and time-consuming. While reinforcement learning (RL) policies trained in simulation present a scalable alternative, effective sim-to-real transfer remains challenging, particularly for tasks that require precise dynamics. To address this, we propose <b>Phys2Real</b>, a real-to-sim-to-real RL pipeline that combines vision-language model (VLM)-inferred physical parameter estimates with interactive adaptation through uncertainty-aware fusion. Our approach consists of three core components: (1) high-fidelity geometric reconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions over physical parameters, and (3) online physical parameter estimation from interaction data. Phys2Real conditions policies on interpretable physical parameters, refining VLM predictions with online estimates via ensemble-based uncertainty quantification. On planar pushing tasks of a T-block with varying CoM and a hammer with an off-center mass distribution, Phys2Real achieves substantial improvements over a domain randomization baseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23% in the challenging top-weighted T-block, and 15% faster task completion for hammer pushing. Ablation studies indicate that the combination of VLM and interaction information is essential for success.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section">
  <div class="container is-max-desktop has-text-centered" style="max-width: 900px;">
    <p class="is-size-4 has-text-weight-medium line-tight">
      We fuse <b>VLM priors</b> with <b><i>interactive</i> online adaptation</b><br>
      to estimate physical parameters and improve sim-to-real manipulation.
    </p>

    <p class="is-size-5 has-text-grey-dark" style="margin-top: 0.5rem;">
      Uncertainty-aware fusion of VLM priors and interaction<br>
      yields near-privileged sim-to-real performance on planar pushing.
    </p>

    <div class="tags is-centered mt-4 tldr-tags">
      <span class="tag is-info is-light is-size-6">
        <b>T-block (weight at top)</b>: 57% vs 24% DR
      </span>
      <span class="tag is-info is-light is-size-6">
        <b>T-block (weight at bottom)</b>: 100% vs 79% DR
      </span>
      <span class="tag is-info is-light is-size-6">
        <b>Hammer</b>: 15% faster than DR
      </span>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata">
        <source src="static/videos/phys2real_vid_final.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <!-- Video description can go here if needed -->
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Physically-Informed Digital Twin</h2>
    <div class="content has-text-justified">
      <p>
        We reconstruct simulation-ready assets from real scenes: segmented 3D Gaussian splats are converted into watertight meshes. This "real-to-sim" step lets policies train on objects that capture geometry relevant to dynamics.
      </p>
    </div>
    <figure class="image publication-banner">
      <img src="static/images/realtosim.png" alt="Pipeline: GSplat segmentation → mesh extraction → watertight simulation mesh">
    </figure>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Policy Learning & Online Adaptation</h2>

    <div class="columns is-multiline is-vcentered">
      <div class="column is-6">
        <div class="content has-text-justified">
          <p>
            We train policies conditioned on interpretable physical parameters (e.g., the object's CoM). Training occurs in three phases: (1) privileged training with accurate parameters, (1.5) noise-aware fine-tuning for robustness, and (2) online adaptation via an uncertainty-aware ensemble that updates the parameter belief during execution.
          </p>
        </div>
      </div>
      <div class="column is-6">
        <figure class="image publication-banner">
          <img src="static/images/phys2real_phases.png" alt="Phase 1–1.5–2 policy training and adaptation">
        </figure>
      </div>
    </div>

    <div class="columns is-multiline is-vcentered mt-4">
      <div class="column is-6">
        <figure class="image publication-banner">
          <img src="static/images/vlm_prompts.png" alt="VLM prior estimation for physical parameters">
        </figure>
      </div>
      <div class="column is-6">
        <div class="content has-text-justified">
          <p>
            A vision-language model provides a <em>prior</em> distribution over the parameters (mean + uncertainty). During interaction, an RMA-style estimator refines this belief. We fuse them using their uncertainties so the policy sees an interpretable estimate that improves over time.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- 1) What we ask -->
<section class="section" id="research-questions">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">How Do VLM Priors, Interaction, and Their Fusion <br>Each Contribute to Sim-to-Real Manipulation?</h2>

    <div class="columns is-multiline">
      <div class="column is-6">
        <div class="box">
          <span class="tag is-link is-light">Q1</span>
          <h3 class="title is-5" style="margin:.5rem 0 0.25rem;">Can accurate physical parameter estimation improve policy performance?</h3>
          Compare a <b>privileged (ground-truth)</b> physics-conditioned policy to domain-randomized and diffusion baselines.</p>
        </div>
      </div>

      <div class="column is-6">
        <div class="box">
          <span class="tag is-link is-light">Q2</span>
          <h3 class="title is-5" style="margin:.5rem 0 0.25rem;">Are VLM priors alone sufficient?</h3>
          <p>Condition the policy solely on <b>VLM-estimated parameters</b>, without interaction, to evaluate whether priors alone enable effective control.</p>
        </div>
      </div>

      <div class="column is-6">
        <div class="box">
          <span class="tag is-link is-light">Q3</span>
          <h3 class="title is-5" style="margin:.5rem 0 0.25rem;">Is interaction alone sufficient?</h3>
          <p>Remove the VLM prior and use only <b>interactive online adaptation</b>.</p>
        </div>
      </div>

      <div class="column is-6">
        <div class="box">
          <span class="tag is-link is-light">Q4</span>
          <h3 class="title is-5" style="margin:.5rem 0 0.25rem;">Does Phys2Real generalize to real-world objects?</h3>
          <p>Apply to <b>image-reconstructed, mesh-free objects</b> (e.g., hammer).</p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- 2) Answer each question with figures -->
<section class="section" id="main-results">
  <div class="container is-max-desktop">

    <!-- Q1–Q3: T-block -->
    <h2 class="title is-3 has-text-centered">Hardware Experiments</h2>

    <div class="box">
      <p class="is-size-6">
        <span class="tag is-link is-light">Q1–Q3</span>
        <b>T-block pushing:</b> Phys2Real <b>consistently outperforms DR and diffusion</b>. With only VLM priors + interaction, we
        approach privileged performance while staying interpretable.
      </p>

      <div class="columns">
        <div class="column">
          <figure class="image publication-banner">
            <img src="static/images/position_error_cdf_weighttop_10cm.jpg" alt="CDF for T-block with top weight">
          </figure>
        </div>
        <div class="column">
          <figure class="image publication-banner">
            <img src="static/images/position_error_cdf_weightbottom_5cm.jpg" alt="CDF for T-block with bottom weight">
          </figure>
        </div>
      </div>

      <p class="has-text-grey is-size-7 has-text-centered" style="margin-top:.25rem;">
        CDF = fraction of trials below each position error. Higher & further left is better.
        Example: 0.8 at 2&nbsp;cm ⇒ 80% of trials ≤ 2&nbsp;cm.
      </p>

      <div class="notification is-light is-info" style="margin-top:.5rem;">
        <b>Takeaway.</b> Neither VLM-only nor interaction-only suffices; <u>their uncertainty-aware fusion</u> closes the gap, approaching the performance of privileged conditioning.
      </div>

      <p class="has-text-grey">
        Top weight: 24% (DR) → 57% (Phys2Real). Bottom weight: 79% → 100%.
      </p>
    </div>

    <!-- Q4: Hammer -->
    <div class="box">
      <p class="is-size-6">
        <span class="tag is-link is-light">Q4</span>
        <b>Hammer (reconstructed object):</b> both methods succeed, but Phys2Real is ~<b>16% faster</b> to completion.
      </p>

      <div class="columns is-centered">
        <div class="column is-6">
          <figure class="image publication-banner">
            <img src="static/images/hammer_completion_time_comparison.png" alt="Hammer completion time comparison">
          </figure>
        </div>
      </div>

      <!-- Side-by-side hammer videos: Phys2Real vs DR (square) -->
      <div class="columns is-centered is-variable is-5"
           style="margin-top:.75rem; max-width:720px; margin-left:auto; margin-right:auto;">
        <!-- Phys2Real -->
        <div class="column is-6">
          <figure class="image publication-banner" style="margin:0;">
            <video
              class="publication-video-el"
              controls
              autoplay
              muted
              loop
              playsinline
              preload="metadata"
              style="width:100%; aspect-ratio:1/1; border-radius:12px; object-fit:contain;">
              <source src="static/images/hammer_phys2real.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </figure>
          <p class="has-text-centered is-size-7" style="margin-top:.4rem;">
            <b>Phys2Real</b> — faster completion
          </p>
        </div>

        <!-- DR -->
        <div class="column is-6">
          <figure class="image publication-banner" style="margin:0;">
            <video
              class="publication-video-el"
              controls
              autoplay
              muted
              loop
              playsinline
              preload="metadata"
              style="width:100%; aspect-ratio:1/1; border-radius:12px; object-fit:contain;">
              <source src="static/images/hammer_dr.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </figure>
          <p class="has-text-centered is-size-7" style="margin-top:.4rem;">
            <b>Domain Randomization (DR)</b>
          </p>
        </div>
      </div>
    </div>

    <!-- Optional: one-line synthesis -->
    <p class="has-text-grey has-text-centered">
      <em>Vision provides a prior; interaction reduces uncertainty for robust sim-to-real control.</em>
    </p>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">
      How VLM Priors and Interactive Online Adaptation <br>Fusion Evolves Over Time
    </h2>
    <div class="content has-text-justified">
      <p>
        Early in each episode, the fused estimate relies on the VLM prior. As interaction accumulates and more contact information becomes available, the ensemble’s epistemic uncertainty shrinks, and the fused estimate converges toward the true physical parameters. This real-time adaptation aligns the policy’s internal physics estimates with the true environment, allowing more accurate and precise control.
      </p>
    </div>

    <!-- Top-weighted block -->
    <h3 class="title is-4 has-text-centered mt-6">T-Block (Weight on Top)</h3>
    <figure class="image publication-banner">
      <video
        class="publication-video-el"
        controls
        autoplay
        muted
        loop
        playsinline
        preload="metadata"
        poster="static/images/fusion_over_time_top_poster.jpg">
        <source src="static/images/episode_20250912-131707_paper_animation.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </figure>

    <!-- Bottom-weighted block -->
    <h3 class="title is-4 has-text-centered mt-6">T-Block (Weight on Bottom)</h3>
    <figure class="image publication-banner">
      <video
        class="publication-video-el"
        controls
        autoplay
        muted
        loop
        playsinline
        preload="metadata"
        poster="static/images/fusion_over_time_bottom_poster.jpg">
        <source src="static/images/episode_20250905-214804_paper_animation.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </figure>
  </div>
</section>

<section class="section" id="takeaway">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Toward Physically Grounded Foundation Models</h2>
    <div class="content has-text-justified" style="max-width:850px;margin:0 auto;">
      <p>
        <b>Phys2Real</b> shows that combining <b>foundation model priors</b> with <b>interactive online adaptation</b>
        yields physically grounded, uncertainty-aware policies that transfer more reliably to the real world.
        By treating large vision models as sources of physical intuition and refining those estimates through real interaction,
        we move toward robotic systems that can <i>understand, predict, and adapt</i> in diverse environments.
      </p>
    </div>
  </div>
</section>

<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <div class="bibtex-header">
      <h2 class="title">BibTeX</h2>
      <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
        <i class="fas fa-copy"></i>
        <span class="copy-text">Copy</span>
      </button>
    </div>
    <pre id="bibtex-code"><code>@article{wang2025phys2real,
  title     = {Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty-Aware Sim-to-Real Manipulation},
  author    = {Wang, Maggie and Tian, Stephen and Swann, Aiden and Shorinwa, Ola and Wu, Jiajun and Schwager, Mac},
  journal   = {arXiv preprint arXiv:2501.XXXXX},
  year      = {2025},
  url       = {https://arxiv.org/abs/2501.XXXXX}
}</code></pre>
  </div>
</section>
<!--End BibTex citation -->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->
<!-- End of Statcounter Code -->

</body>
</html>